(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[818],{4478:function(e,n,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/projects",function(){return t(8289)}])},3403:function(e,n,t){"use strict";t.d(n,{Z:function(){return d}});var o=t(5893),r=(t(7294),t(1664)),i=t(4184),a=t.n(i),s=t(5332),l=t.n(s),u=t(1163),c=[{link:"/",text:"Home"},{link:"/about",text:"About"},{link:"/projects",text:"Projects"}];function d(e){var n=e.children,t=(0,u.useRouter)().asPath;return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)("nav",{className:l().nav,children:c.map((function(e){return(0,o.jsx)(r.default,{href:e.link,children:(0,o.jsx)("a",{className:a()(t===e.link&&l().active),children:e.text})},e.link)}))}),(0,o.jsx)("main",{children:n})]})}},8289:function(e,n,t){"use strict";t.r(n),t.d(n,{default:function(){return T}});var o=t(5893),r=t(7294),i=t(3403),a=t(6121),s=t(3483),l=t(4184),u=t.n(l),c=t(7331),d=t.n(c);function m(e){var n=e.activeId,t=e.headingsTree;return(0,o.jsx)("ul",{className:d().list,children:t.map((function(e){return(0,o.jsxs)("li",{className:u()(d().entry,e.id===n&&d().active),children:[(0,o.jsx)("a",{href:"#".concat(e.id),children:e.text}),e.children.length>0&&(0,o.jsx)(m,{activeId:n,headingsTree:e.children})]},e.id)}))})}var h=t(3197),f=t.n(h);function p(e,n){var t=(0,r.useRef)({});(0,r.useEffect)((function(){var o=new IntersectionObserver((function(n){return function(e,n,t){var o=!0,r=!1,i=void 0;try{for(var a,s=n[Symbol.iterator]();!(o=(a=s.next()).done);o=!0){var l=a.value;e.current[l.target.id]=l}}catch(c){r=!0,i=c}finally{try{o||null==s.return||s.return()}finally{if(r)throw i}}var u=Object.values(e.current).filter((function(e){return e.isIntersecting}));u.length>0&&t(u[0].target.id)}(t,n,e)}),{rootMargin:"75px 0px 0px 0px"});return function(e,n){var t=!0,o=!1,r=void 0;try{for(var i,a=n[Symbol.iterator]();!(t=(i=a.next()).done);t=!0){var s=i.value;e.observe(s)}}catch(l){o=!0,r=l}finally{try{t||null==a.return||a.return()}finally{if(o)throw r}}}(o,n),function(){return o.disconnect()}}),[e,n])}function b(e){return(0,r.useMemo)((function(){return function(e){var n=[],t=!0,o=!1,r=void 0;try{for(var i,a=e[Symbol.iterator]();!(t=(i=a.next()).done);t=!0){var s=i.value,l=s.innerText,u=s.id,c=s.nodeName,d={children:[],id:u,text:l};"H1"===c?n.push(d):"H2"===c&&n.length>0&&n[n.length-1].children.push(d)}}catch(m){o=!0,r=m}finally{try{t||null==a.return||a.return()}finally{if(o)throw r}}return n}(e)}),[e])}function g(e){var n,t=e.children,i=(0,r.useState)(),a=i[0],s=i[1],l=(n=a,(0,r.useMemo)((function(){return n?Array.from(document.querySelectorAll("h1, h2")):[]}),[n])),u=(0,r.useState)(),c=u[0],d=u[1],h=b(l);return p(d,l),(0,o.jsxs)("div",{className:f().container,children:[(0,o.jsx)("div",{className:f().content,ref:function(e){return s(e)},children:t}),(0,o.jsx)("nav",{className:f().toc,"aria-label":"Table of contents",children:(0,o.jsx)(m,{activeId:c,headingsTree:h})})]})}var y=t(4441),w=t(305),v=t(5968),_=t(7385),k=t.n(_);function x(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function j(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{},o=Object.keys(t);"function"===typeof Object.getOwnPropertySymbols&&(o=o.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),o.forEach((function(n){x(e,n,t[n])}))}return e}function I(e,n){if(null==e)return{};var t,o,r=function(e,n){if(null==e)return{};var t,o,r={},i=Object.keys(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)t=i[o],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}function T(){return(0,o.jsx)(a.D,{remarkPlugins:[v.Z],rehypePlugins:[w.Z],components:{code:function(e){var n=e.inline,t=e.className,r=e.children,i=I(e,["inline","className","children"]),a=/language-(\w+)/.exec(t||"");return!n&&a?(0,o.jsx)(s.Z,j({style:y.Z,language:a[1],PreTag:"div"},i,{children:String(r).replace(/\n$/,"")})):(0,o.jsx)("code",j({className:u()(t,k().inlineCode)},i,{children:r}))}},children:'# Lazyjson\n\nGiven my nature, I like to continuously improve my skills. And a great way to do that is to learn how existing things are created. That\'s why one day I decided, I wanted to understand parsers. It always fascinated me, how the meaning of something so complicated like a (programming) language could be parsed, and transformed into something different. For example a compiler, or an interpreter. I didn\'t want to create a complete programming language. That would have taken way too much time to get anything useful out of it. So I decided to try and create a JSON parser.\nI re-wrote the parser multiple times, even in several languages. First, I used TypeScript, simply because that\'s what I knew best. After writing a first, rudimentary implementation, I refactored the code a couple of times, until I decided, I wanted to try out Go. So I started writing it in Go, and I recently learned about WebAssembly, so I tried compiling it to that. After a couple of issues, I left the project on the side for some time. But then I learned about another language that could be compiled into WebAssembly. Additionally, I keep hearing good things about this language, supposedly, It\'s even going to be inside the Linux kernel. So I chose to re-write again. This time in rust.\n\nIf you don\'t care about the details, just head over the [interactive demo](/demos/lazyjson)! Otherwise, read on.\n\nThe parser consists of two main components, the tokenizer, and the tree builder.\n\n## Tokenizer\n\nThe tokenizer is responsible for figuring out what each part of the text is. Just simple checks, for example: "Is this a number", "is this a comma", "is this whitespace", and so on.\n\nThis step is called tokenization or often [lexical analysis](https://en.wikipedia.org/wiki/Lexical_analysis).\n\nI implemented each "consumer" in its own, separate function. A consumer in my context just takes the input, and checks if it can identify the contents, for example, here is my operator consumer:\n\n```rust\npub fn operator_consumer(inp: &mut CharQueue) -> Result<Option<Token>, TokenizationErr> {\n    let c = inp.peek().ok_or(TokenizationErr::new_out_of_bounds())?;\n\n    let tok = match c {\n        \':\' => Token::new_json_assignment_op(inp.idx()),\n        \'=\' => Token::new_equal_assignment_op(inp.idx()),\n        _ => return Ok(None),\n    };\n\n    inp.advance_by(1);\n\n    Ok(Some(tok))\n}\n```\n\nAs you can see, well at least if you know rust a little bit, this function can return either an error or an optional token.\n\nThen, in the main tokenization function, all of the consumers are combined. It loops over every one of them. If a function returns a token, we can move on to the next piece of text. If it returns `None`, the text was not identified as consumable by this consumer, and we can check it with the next one.\n\n```rust\npub fn tokenize(inp: &str, config: &Config) -> Result<Vec<Token>, TokenizationErr> {\n    if inp.is_empty() {\n        return Err(TokenizationErr::new_no_inp());\n    }\n\n    let consumers: &[&Consumer] = &[\n        &line_comment_consumer,\n        &whitespace_consumer,\n        &delimiter_consumer,\n        &keyword_literal_consumer,\n        &number_literal_consumer,\n        &operator_consumer,\n        &separator_consumer,\n        &string_literal_consumer,\n    ];\n\n    let mut queue = CharQueue::new(inp);\n    let mut toks = Vec::new();\n\n    \'o: while queue.has_remaining() {\n        for consumer in consumers {\n            let tok = consumer(&mut queue)?;\n\n            if let Some(tok) = tok {\n                // Omit unnecessary whitespace tokens\n                if tok.typ == TokenType::WhitespaceLiteral {\n                    continue \'o;\n                }\n\n                // Line comments are currently not supported by the treebuilder.\n                // So if they are allowed, we omitted them, and otherwise throw an\n                // error.\n                if tok.typ == TokenType::LineComment {\n                    if config.allow_line_comments {\n                        continue \'o;\n                    }\n\n                    return Err(TokenizationErr::new_line_comments_not_allowed(\n                        tok.from, tok.to,\n                    ));\n                }\n\n                toks.push(tok);\n                continue \'o;\n            }\n        }\n\n        panic!("{:?} was not consumed", queue.peek());\n    }\n\n    Ok(toks)\n}\n```\n\n## Tree builder\n\nAfter obtaining a list of tokens, the tree builder can now check for valid patterns, for example: `[`, `false`, `]`, and build a tree of nodes out of it. The mentioned example would result in the following node structure: `ArrayNode` -> `entries` -> `BoolNode` -> `false`.\n\nThe tree builder works fundamentally in the same way as the tokenizer. It combines a set of consumers, and checks if they could consume a given token composition.\n\n```rust\npub fn number_consumer(\n    inp: &mut Queue<Token>,\n    _: &Rc<VarDict>,\n    _: &Config,\n) -> Result<Option<Node>, TreebuilderErr> {\n    if t.typ != TokenType::NumberLiteral {\n        return Ok(None);\n    }\n\n    let i = inp.idx();\n    let t = inp.next().unwrap();\n\n    Ok(Some(NumberNode::new(i, t.val.clone()).into()))\n}\n```\n\n```rust\n/// Consumes all possible forms of "value constellations". For example simple\n/// numbers (`1`), or arrays (`[1, 2]`), and so on. This consumer combines other\n/// "sub-consumers" to achieve this behavior.\npub fn value_consumer(\n    toks: &mut Peekable<TokenIndices>,\n    var_dict: &Rc<VarDict>,\n    config: &Config,\n) -> Result<Option<Node>, TreebuilderErr> {\n    let consumers: &[&Consumer] = &[\n        &array_consumer,\n        &keyword_consumer,\n        &variable_usage_consumer,\n        &number_consumer,\n        &object_consumer,\n        &string_consumer,\n    ];\n\n    for consumer in consumers {\n        let res = consumer(toks, var_dict, config)?;\n\n        if res.is_some() {\n            return Ok(res);\n        }\n    }\n\n    Ok(None)\n}\n```\n\nSo this is the basic functionality of a JSON parser complete. But I wanted to add some of my own features. Mostly "fixing" things I always found to be annoying about the JSON format.\n\n## Trailing commas\n\nThe JSON format does not permit having trailing commas, this can be most annoying when moving entries around. I\'ve also seen that another argument for trailing comma that I\'ve read, is that to add an entry, you would need the add the entry itself, and a comma on the previous line. In source control, this will show up as a two-line change, which it isn\'t. \n\nSo I went and added a config, as I wanted to be able to turn this feature on and off. And started checking for trailing commas:\n\n```rust\nconsume_val_sep(inp)?;\n\n// Check if the next token is an object close, if yes, we have a trailing\n// separator.\nif consume_obj_cls(inp, opn_i)? {\n    if !config.allow_trailing_commas {\n        return Err(TreebuilderErr::new_trailing_sep(inp.idx() - 2));\n    }\n\n    return Ok(Some(ObjectNode::new(opn_i, inp.idx(), entries).into()));\n}\n```\n\nI also added a custom error message for it, so when the option is disabled (no trailing commas allowed), the following will appear:\n\n```text\nexpected the next value or close (trailing separator not allowed), line: 1, char: 14\n\n{"foo": "bar",}\n             ^\n```\n\nSpeaking of error messages, I spend quite a lot of time making them as useful as possible.\n\n## Error messages\n\nThe parser was designed to tell the user what went wrong. For me, this was important, as I often found the error messages of the JavaScript JSON parser quite useless.\n\n### Let\'s look at a few error messages:\n\n#### Forgot a `,` inside an array:\n\n```text\nexpected a `,` but received a `KeywordLiteral`, line: 1, char: 8\n\n[false true]\n       ^^^^\n```\n\n#### Missing quotes around an object key (to be fair, this one isn\'t all that obvious, but still, it marks what is wrong):\n\n```text\nexpected a `StringLiteral` but received a `KeywordLiteral`, line: 1, char: 2\n\n{key: "val"}\n ^^^\n```\n\n#### Forgot to close the object:\n\n```text\nobject was not terminated, line: 1, char: 1\n\n{"foo": "bar"\n^\n```\n\nNext up: line comments.\n\n## Line comments\n\nProvided the correct flag is set to true, the parser supports line comments. Well, the tokenizer just ignores them.\nIf the flag is not set, the output will be:\n\n```text\nline comments not allowed\n```\n\n## Emitting\n\nGiven that I have a complete tree of nodes, I implemented the opposite of parsing, emitting! I\'ve implemented this somewhat limited. The emitter is not configurable at all, but that wasn\'t its purpose anyway. I mainly implemented it, so that one can see some sort of output, instead of just "parsed successfully". Also, the next feature would be hard to demonstrate without this.\n\nSo let\'s look at the maybe biggest feature I added to JSON.\n\n## Variables\n\nYep, I added variables. Nothing really to say about this, except that they can be defined inside container nodes (arrays, and objects), and the scope of them is bound to the node it is defined in.\nLet\'s jump into some examples:\n\nThis is a valid variable declaration:\n\n```text\n{let foo = 10}\n```\n\nAnd would simply output:\n\n```json\n{}\n```\n\nActually using the variable:\n\n```text\n{\n    let foo = "bar",\n    "foobar": foo\n}\n```\n\nThe output:\n\n```json\n{\n    "foobar": "bar"\n}\n```\n\nNested variables are supported as well:\n\n```text\n{\n    let port = 3000,\n    let apiArgs = ["run", port],\n    let webArgs = ["bind", port],\n    "services": {\n        "api": apiArgs,\n        "web": webArgs\n    }\n}\n```\n```json\n{\n    "services": {\n        "api": [\n            "run",\n            3000\n        ],\n        "web": [\n            "bind",\n            3000\n        ]\n    }\n}\n```\n\n## Conclusion\n\nThis is, like all the others, a learning project, and not intended to be *actually* used. There are some bugs, some errors, and definitely some improvements that could be made. But if you made it this far, definitely check out the [demo](/demos/lazyjson)!'})}T.getLayout=function(e){return(0,o.jsx)(i.Z,{children:(0,o.jsx)(g,{children:e})})}},7331:function(e){e.exports={list:"tableOfContents_list__b8d6s",entry:"tableOfContents_entry__IfVwq",active:"tableOfContents_active__c_CsF"}},5332:function(e){e.exports={nav:"navBarLayout_nav__R8NZ9",active:"navBarLayout_active__hWBct"}},3197:function(e){e.exports={container:"tableOfContentsLayout_container__EzUwh",toc:"tableOfContentsLayout_toc__ZjzLh",content:"tableOfContentsLayout_content__KYkIj"}},7385:function(e){e.exports={inlineCode:"projects_inlineCode__PRmXF"}}},function(e){e.O(0,[685,594,774,888,179],(function(){return n=4478,e(e.s=n);var n}));var n=e.O();_N_E=n}]);